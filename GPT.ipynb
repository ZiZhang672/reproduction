{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3380685c-bb68-429c-8d32-8d6ce1b050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a369aa-9e74-4557-b83b-493ea2b72744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, hidden_dim, num_head, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_q = nn.Linear(q_dim, hidden_dim)\n",
    "        self.W_k = nn.Linear(k_dim, hidden_dim)\n",
    "        self.W_v = nn.Linear(v_dim, hidden_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, valid_len = None, causal = True):\n",
    "        head_dim = self.hidden_dim // self.num_head\n",
    "        B, Lq, _ = query.shape\n",
    "        _, Lk, _ = key.shape\n",
    "        _, Lv, _ = value.shape\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        Q = Q.reshape(B, Lq, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        K = K.reshape(B, Lk, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        V = V.reshape(B, Lv, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        scale = Q @ K.transpose(2,3) / (head_dim**0.5)\n",
    "        if valid_len != None:\n",
    "            mask = torch.arange(Lk)[None, None, None, : ] >= valid_len[:, None, None, None]\n",
    "            scale = scale.masked_fill(mask, -1e6)\n",
    "        if causal:\n",
    "            causal_mask = torch.zeros(Lq,Lk).triu(1).bool()\n",
    "            scale = scale.masked_fill(causal_mask[None, None, :,:], -1e6)\n",
    "        weight = F.softmax(scale, dim = -1) \n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ V\n",
    "        O = out.reshape(B, Lq, self.hidden_dim)\n",
    "        return self.W_o(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa2f4b1-b50e-4b4d-beaa-1cb05f3907c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return  self.fc2(self.dropout(F.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd5a36b-4bdd-4b0a-93df-50e7dc985748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, dropout)\n",
    "        self.mlp = FeedForward(hidden_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x, x, x, causal = True)\n",
    "        x = self.norm1(x + att)\n",
    "        m = self.mlp(x)\n",
    "        x = self.norm2(x + m)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45addc60-c454-4b81-9cb2-ddee901025a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layer, num_head, ff_dim, max_len=1000, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.tokenEmbedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positionEmbedding = nn.Embedding(max_len, hidden_dim)\n",
    "        self.decoder = nn.ModuleList([GPTBlock(hidden_dim, num_head, ff_dim, dropout) for _ in range(num_layer)])\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, tokens):\n",
    "        B, L = tokens.shape\n",
    "        pos = torch.arange(L).unsqueeze(0).expand(B, L)\n",
    "        X = self.tokenEmbedding(tokens) + self.positionEmbedding(pos)\n",
    "        for layer in self.decoder:\n",
    "            X = layer(X)\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118119e-26d3-433b-a7da-a7e541764d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
