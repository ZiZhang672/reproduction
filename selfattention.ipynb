{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3380685c-bb68-429c-8d32-8d6ce1b050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d45d4e00-6e4b-4a81-bb98-819712e55c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, hidden_dim, num_head, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_q = nn.Linear(q_dim, hidden_dim)\n",
    "        self.W_k = nn.Linear(k_dim, hidden_dim)\n",
    "        self.W_v = nn.Linear(v_dim, hidden_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, valid_len = None):\n",
    "        head_dim = self.hidden_dim // self.num_head\n",
    "        B, L, _ = query.shape\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        Q = Q.reshape(B, L, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        K = K.reshape(B, L, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        V = V.reshape(B, L, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        scale = Q @ K.transpose(2,3) / (head_dim**0.5)\n",
    "        if valid_len != None:\n",
    "            mask = torch.arange(L)[None, None, None, : ] >= valid_len[:, None, None, None]\n",
    "            scale = scale.masked_fill(mask, -1e6)\n",
    "        weight = F.softmax(scale, dim = -1) \n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ V\n",
    "        O = out.reshape(B, L, self.hidden_dim)\n",
    "        return self.W_o(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ad71190-8769-408f-bce3-f5954fa69a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "num_head = 5\n",
    "attention = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, 0.5)\n",
    "attention.eval()\n",
    "batchsize = 2\n",
    "num_queries = 4\n",
    "X = torch.ones((batchsize, num_queries, hidden_dim))\n",
    "valid_len = torch.tensor([3,2])\n",
    "attention(X, X, X, valid_len).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b0eaae-023f-4433-9208-e3e929533bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout, max_len = 1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, hidden_dim))\n",
    "        X = torch.arange(max_len).reshape(-1,1)/torch.pow(10000, torch.arange(0, hidden_dim, 2)/hidden_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1],:]\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c409906-e969-450e-b0f1-d094f79f08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 100\n",
    "dropout = 0.5\n",
    "tokens = 50\n",
    "posEncoding = PositionalEncoding(encoder_dim, dropout)\n",
    "X = posEncoding(torch.ones(1, tokens, encoder_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7165ba6-5740-4ee8-b35a-d8d99dd5fc86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
