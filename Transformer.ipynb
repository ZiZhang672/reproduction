{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3380685c-bb68-429c-8d32-8d6ce1b050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d45d4e00-6e4b-4a81-bb98-819712e55c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, hidden_dim, num_head, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_q = nn.Linear(q_dim, hidden_dim)\n",
    "        self.W_k = nn.Linear(k_dim, hidden_dim)\n",
    "        self.W_v = nn.Linear(v_dim, hidden_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, valid_len = None, causal = False):\n",
    "        head_dim = self.hidden_dim // self.num_head\n",
    "        B, Lq, _ = query.shape\n",
    "        _, Lk, _ = key.shape\n",
    "        _, Lv, _ = value.shape\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        Q = Q.reshape(B, Lq, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        K = K.reshape(B, Lk, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        V = V.reshape(B, Lv, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        scale = Q @ K.transpose(2,3) / (head_dim**0.5)\n",
    "        if valid_len != None:\n",
    "            mask = torch.arange(Lk)[None, None, None, : ] >= valid_len[:, None, None, None]\n",
    "            scale = scale.masked_fill(mask, -1e6)\n",
    "        if causal:\n",
    "            causal_mask = torch.zeros(Lq,Lk).triu(1).bool()\n",
    "            scale = scale.masked_fill(causal_mask[None, None, :,:], -1e6)\n",
    "        weight = F.softmax(scale, dim = -1) \n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ V\n",
    "        O = out.reshape(B, Lq, self.hidden_dim)\n",
    "        return self.W_o(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ad71190-8769-408f-bce3-f5954fa69a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "num_head = 5\n",
    "attention = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, 0.5)\n",
    "attention.eval()\n",
    "batchsize = 2\n",
    "num_queries = 4\n",
    "X = torch.ones((batchsize, num_queries, hidden_dim))\n",
    "valid_len = torch.tensor([3,2])\n",
    "attention(X, X, X, valid_len).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b0eaae-023f-4433-9208-e3e929533bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout, max_len = 1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, hidden_dim))\n",
    "        X = torch.arange(max_len).reshape(-1,1)/torch.pow(10000, torch.arange(0, hidden_dim, 2)/hidden_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1],:]\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c409906-e969-450e-b0f1-d094f79f08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 100\n",
    "dropout = 0.5\n",
    "tokens = 50\n",
    "posEncoding = PositionalEncoding(encoder_dim, dropout)\n",
    "X = posEncoding(torch.ones(1, tokens, encoder_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7165ba6-5740-4ee8-b35a-d8d99dd5fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return  self.fc2(self.dropout(F.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cd5a36b-4bdd-4b0a-93df-50e7dc985748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, dropout)\n",
    "        self.mlp = FeedForward(hidden_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x, x, x)\n",
    "        x = self.norm1(x + att)\n",
    "        m = self.mlp(x)\n",
    "        x = self.norm2(x + m)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df28e0d8-bfd3-4f7e-8c28-0a3696d9d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention1 = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, dropout)\n",
    "        self.attention2 = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, dropout)\n",
    "        self.mlp = FeedForward(hidden_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x, input_embed):\n",
    "        att1 = self.attention1(x, x, x, causal = True)\n",
    "        x = self.norm1(x + att1)\n",
    "        att2 = self.attention2(x, input_embed, input_embed)\n",
    "        x = self.norm2(x + att2)\n",
    "        m = self.mlp(x)\n",
    "        x = self.norm3(x + m)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0932a795-859b-4146-a579-5128fb9cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layer, num_head, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.PE = PositionalEncoding(hidden_dim, dropout)\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(hidden_dim, num_head, ff_dim, dropout) for _ in range(num_layer)])\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(hidden_dim, num_head, ff_dim, dropout) for _ in range(num_layer)])\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, inputs, outputs):\n",
    "        input_embed = self.embed(inputs)\n",
    "        input_embed = self.PE(input_embed)\n",
    "        for layer in self.encoder:\n",
    "            input_embed = layer(input_embed)\n",
    "        output_embed = self.embed(outputs)\n",
    "        output_embed = self.PE(output_embed)\n",
    "        for layer in self.decoder:\n",
    "            output_embed = layer(output_embed, input_embed)\n",
    "        return self.fc_out(output_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84afcab-17d0-4a75-bcf4-5a392e35bd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45addc60-c454-4b81-9cb2-ddee901025a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
