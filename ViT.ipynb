{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3380685c-bb68-429c-8d32-8d6ce1b050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98a369aa-9e74-4557-b83b-493ea2b72744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, hidden_dim, num_head, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_q = nn.Linear(q_dim, hidden_dim)\n",
    "        self.W_k = nn.Linear(k_dim, hidden_dim)\n",
    "        self.W_v = nn.Linear(v_dim, hidden_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, valid_len = None, causal = False):\n",
    "        head_dim = self.hidden_dim // self.num_head\n",
    "        B, Lq, _ = query.shape\n",
    "        _, Lk, _ = key.shape\n",
    "        _, Lv, _ = value.shape\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        Q = Q.reshape(B, Lq, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        K = K.reshape(B, Lk, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        V = V.reshape(B, Lv, self.num_head, head_dim).permute(0,2,1,3)\n",
    "        scale = Q @ K.transpose(2,3) / (head_dim**0.5)\n",
    "        if valid_len != None:\n",
    "            mask = torch.arange(Lk)[None, None, None, : ] >= valid_len[:, None, None, None]\n",
    "            scale = scale.masked_fill(mask, -1e6)\n",
    "        if causal:\n",
    "            causal_mask = torch.zeros(Lq,Lk).triu(1).bool()\n",
    "            scale = scale.masked_fill(causal_mask[None, None, :,:], -1e6)\n",
    "        weight = F.softmax(scale, dim = -1) \n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ V\n",
    "        O = out.reshape(B, Lq, self.hidden_dim)\n",
    "        return self.W_o(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aa2f4b1-b50e-4b4d-beaa-1cb05f3907c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return  self.fc2(self.dropout(F.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cd5a36b-4bdd-4b0a-93df-50e7dc985748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, ff_dim, dropout):\n",
    "        super().__init()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_head, dropout)\n",
    "        self.mlp = FeedForward(hidden_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x, x, x)\n",
    "        x = self.norm1(x + att)\n",
    "        m = self.mlp(x)\n",
    "        x = self.norm2(x + m)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a84afcab-17d0-4a75-bcf4-5a392e35bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layer, num_head, ff_dim, max_len = 1000, dropout = 0.5):\n",
    "         super().__init__()\n",
    "         self.num_layer = num_layer\n",
    "         self.tokenEmbedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "         self.positionEmbedding = nn.Embedding(max_len, hidden_dim)\n",
    "         self.segEmbedding = nn.Embedding(2, hidden_dim)\n",
    "         self.encoder = nn.ModuleList([EncoderBlock(hidden_dim, num_head, ff_dim, dropout) for _ in range(num_layer)])\n",
    "         self.norm = nn.LayerNorm(hidden_dim)\n",
    "         self.mlm = nn.Linear(hidden_dim, vocab_size)\n",
    "         self.nsp = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, tokens, seg):\n",
    "          B, L = tokens.shape\n",
    "          pos = torch.arange(L).unsqueeze(0).expand(B, L)\n",
    "          X = self.tokenEmbedding(tokens) + self.positionEmbedding(pos) + self.segEmbedding(seg)\n",
    "          for layer in self.encoder:\n",
    "              X = layer(X)\n",
    "          X = self.norm(X)\n",
    "          mlm_head = self.mlm(X)\n",
    "          nsp_head = self.nsp(X[:, 0, :])\n",
    "          return mlm_head, nsp_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45addc60-c454-4b81-9cb2-ddee901025a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size = 224, patch_size = 16, channel = 3, hidden_dim = 768):\n",
    "        super().__init__()\n",
    "        self.hidden_dim =hidden_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patchs = (image_size//patch_size)**2\n",
    "        self.patch_dim = self.patch_size**2*channel\n",
    "        self.proj = nn.Linear(self.patch_dim, hidden_dim)\n",
    "    def forward(self, X):\n",
    "        B, C, H, W = X.shape\n",
    "        X = X.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        X = X.contiguous().view(B, C, self.num_patchs, -1)\n",
    "        X = X.permute(0, 2, 1, 3).contiguous().view(B, -1, self.patch_dim)\n",
    "        return self.proj(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafd1270-e7c1-4b9d-a114-0f10a0437511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, channel, hidden_dim, num_layer, num_head, ff_dim, num_class, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.patchembed = PatchEmbedding(image_size, patch_size, channel, hidden_dim)\n",
    "        self.num_layer = num_layer\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.clsToken = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.posEmbed = nn.Parameter(torch.zeros(1, (image_size//patch_size)**2 + 1, hidden_dim))\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(hidden_dim, num_head, ff_dim, dropout) for _ in range(num_layer)])\n",
    "        self.mlp = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, X):\n",
    "        B = X.size(0)\n",
    "        X = self.patchembed(X)\n",
    "        cls = self.clsToken.expand(B, -1, -1)\n",
    "        X = torch.cat((cls, X), dim = 1)\n",
    "        pos = self.posEmbed[:, :X.size(1), :]\n",
    "        X = X + pos\n",
    "        for layer in self.encoder:\n",
    "            X = layer(X)\n",
    "        return self.mlp(X[:, 0, :])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37204490-0d4b-468e-8233-2ced8f633b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
